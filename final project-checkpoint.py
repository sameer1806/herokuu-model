#!/usr/bin/env python
# coding: utf-8

# **The accompanying dataset has data about leads generated by a Travel Aggregator in the last six months along with information about them and whether they ended up converting (ie – booking a trip). You need to use this dataset to build a propensity model that the sales team can use to target new leads.**
# 
# **Data Description :**
# 
# **RowNumber** – row identifier
# 
# **LeadId** – id for the lead
# 
# **Surname** – prospect surname
# 
# **months_since_lead_gen** – number of months since lead was generated
# 
# **lead_gen_channel** – channel through which lead was generated 
# 
# **Gender** – gender of customer 
# 
# **Age** - age of customer
# 
# **City** – city of customer
# 
# **airline_loyalty_tier** – whether customer belongs to any airline loyalty program and if so which tier 
# 
# **no_of_family_members** – number of family members of customer
# 
# **is_6M_enquiry** – whether customer has made a holiday enquiry in the last six months
# 
# **is_3M_active** – whether customer has been active on the website / app in the last six months
# 
# **Converted_y_N** – whether booked or not
# 

# ## import libraries

# In[126]:


import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
plt.rc("font", size=14)
import seaborn as sns
sns.set(style="white")
sns.set(style="whitegrid", color_codes=True)
from matplotlib.pyplot import figure

import statsmodels.api as sm
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, RandomizedSearchCV, StratifiedKFold
from sklearn.feature_selection import RFE
from imblearn.over_sampling import SMOTE

from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn import metrics
from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, roc_auc_score 
from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report

import pickle


# In[2]:


df=pd.read_csv("Data.csv")


# In[3]:


df.head()


# In[4]:


df.shape


# In[5]:


df.dtypes


# In[6]:


df.isnull().sum()


# **no null values**

# In[7]:


df.describe()


# **DATA ANALYSIS**

# INTIAL ANALYSIS
# 
# - From the initial observations We can infer that the 'RowNumber' column is basically a row identifier and does not play any significant role in the analysis
# - LeadId is also another identification column used for identifying the customers uniquely. We cannot use it for Modelling purpose.
# - Surname is also another identification column used for identifying the customers uniquely. We cannot use it for Modelling purpose.
# 
# - The categorical Independent variables are :
#     - months_since_lead_gen
#     - lead_gen_channel
#     - Gender
#     - City
#     - airline_loyalty_tier
#     - no_of_family_members	
#     - is_6M_enquiry	
#     - is_3M_active
# - The continuous Independent variables are :
#     - Age
#     
# - The target variable is 'Converted_y_N'

# ## analysis of target

# In[8]:


df["Converted_y_N"].value_counts()


# In[9]:


sns.countplot(df["Converted_y_N"])
plt.savefig("a.png")


# ## here we have imbalance dataset
# - 0(non converted)=80%
# - 1(converted)  = 20%

# ## lead gen month analysis

# In[10]:


df.months_since_lead_gen.value_counts()


# In[11]:


print(df[df.Converted_y_N==1].months_since_lead_gen.value_counts())


# In[12]:


df[df.Converted_y_N==0].months_since_lead_gen.value_counts()


# In[13]:


sns.countplot(x=df.Converted_y_N,hue=df.months_since_lead_gen)
plt.title('Converted leads form the Month')
plt.xlabel('Months since lead generated')
plt.ylabel('Frequency of Converted leads')
plt.show()


# - from above plot both maximum number of converted  and not-converted leads  took place in the month of 4
# - month wise approach is not good for analysis.
# - so, we can get conversion rate of each month to identify the best month of conversion.

# In[14]:


print(df[df.months_since_lead_gen==1].Converted_y_N.value_counts(normalize=True))


# In[15]:


print(df[df.months_since_lead_gen==2].Converted_y_N.value_counts(normalize=True))


# In[16]:


print(df[df.months_since_lead_gen==3].Converted_y_N.value_counts(normalize=True))


# In[17]:


print(df[df.months_since_lead_gen==4].Converted_y_N.value_counts(normalize=True))


# In[18]:


print(df[df.months_since_lead_gen==5].Converted_y_N.value_counts(normalize=True))


# In[19]:


print(df[df.months_since_lead_gen==6].Converted_y_N.value_counts(normalize=True))


# In[20]:


table=pd.crosstab(df.months_since_lead_gen,df.Converted_y_N)
table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)
plt.title('Stacked Bar Chart for months of lead generation vs Converted leads')
plt.xlabel('month of lead genneration')
plt.ylabel('Proportion of Converted leads')
plt.savefig("month vs target")


# - in first month the lead conversion have 100%probability
# - after that it will reduce the conversion rate overall it has 20% average.

# ## lead generated channels analysis

# In[21]:


print(df[df.Converted_y_N==1].lead_gen_channel.value_counts(normalize=True))


# In[22]:


print(df[df.Converted_y_N==0].lead_gen_channel.value_counts(normalize=True))


# In[23]:


sns.countplot(x=df.Converted_y_N,hue=df.lead_gen_channel)


# - it doesn't clearly interpret the data.
# - we get conversion rate of each channel to say best channel for conversion.

# In[24]:


print(df[df.lead_gen_channel=='Direct'].Converted_y_N.value_counts(normalize=True))


# In[25]:


print(df[df.lead_gen_channel=='Networks'].Converted_y_N.value_counts(normalize=True))


# In[26]:


print(df[df.lead_gen_channel=='Organic'].Converted_y_N.value_counts(normalize=True))


# In[27]:


table=pd.crosstab(df.lead_gen_channel,df.Converted_y_N)
table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)
plt.title('Stacked Bar Chart for  lead generation channel vs Converted leads')
plt.xlabel('leads generated through channels')
plt.ylabel('Proportion of Converted leads')
plt.savefig("channel vs target")


# - from above values we know the direct channel has 2 times more chances of converting so, we use this channel for customer value generation.

# ## Gender analysis

# In[28]:


print(df[df.Converted_y_N==1].Gender.value_counts())


# In[29]:


print(df[df.Converted_y_N==0].Gender.value_counts())


# In[30]:


sns.countplot(x=df.Converted_y_N,hue=df.Gender)


# - in count wise female has more conversion rate than male.
# - but its not enough for analysis
# - lets check each gender prob value.

# In[31]:


print(df[df.Gender=='Male'].Converted_y_N.value_counts(normalize=True))


# In[32]:


print(df[df.Gender=='Female'].Converted_y_N.value_counts(normalize=True))


# In[33]:


table=pd.crosstab(df.Gender,df.Converted_y_N)
table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)
plt.title('Stacked Bar Chart for Gender vs Converted leads')
plt.xlabel('Gender')
plt.ylabel('Proportion of Converted leads')
plt.savefig("gender vs target")


# - female has 50% more probability to conversion

# ## age analysis

# In[34]:


df.Age.describe()


# In[35]:


age_band=[]
age=df.Age
for i in age:
    if i < 24:
        age_band.append("teenagers")
    elif i >= 24 and i <=64 :
        age_band.append("adults")
    else:
        age_band.append("elderly")


# In[36]:


df["age_band"]=age_band


# In[37]:


df.head()


# In[38]:


df["Age"].value_counts(bins=3)


# In[39]:


print(df[df.Converted_y_N==0].age_band.value_counts())


# In[40]:


print(df[df.Converted_y_N==1].age_band.value_counts())


# In[41]:


print(df[df.Converted_y_N==0].age_band.value_counts())


# In[42]:


sns.countplot(x=df.Converted_y_N,hue=df["age_band"])


# - from above reference both converted & not converted rate high for adults
# - we doesn't conclude only with this.
# - we check individual conversion rate in age.

# In[43]:


print(df[df["age_band"]=='adults'].Converted_y_N.value_counts(normalize=True))


# In[44]:


print(df[df["age_band"]=='teenagers'].Converted_y_N.value_counts(normalize=True))


# In[45]:


print(df[df["age_band"]=='elderly'].Converted_y_N.value_counts(normalize=True))


# - nothing useful in age.
# 

# In[46]:


sns.distplot(df.Age)
plt.savefig("age dist")


# - age column is not uniformally distributed its slightly right skewed.
# - lets we check outliers for age.

# In[47]:


sns.boxplot(df['Age'])
plt.savefig("age box")


# - outliers are present in age field

# ## City analysis

# In[48]:


print(df[df.Converted_y_N==1].City.value_counts())


# In[49]:


print(df[df.Converted_y_N==0].City.value_counts())


# In[50]:


sns.countplot(x=df.Converted_y_N,hue=df.City)


# In[51]:


df.groupby("City")["Converted_y_N"].mean().sort_values(ascending=False)


# In[52]:


a=df.City.unique()


# In[53]:


for i in a:
    print(i)
    print(df[df.City==i].Converted_y_N.value_counts(normalize=True))
    print("\n")
    


# In[54]:


plt.figure(figsize=(10,10))
table=pd.crosstab(df.City,df.Converted_y_N)
table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)
plt.title('Stacked Bar Chart for City vs Converted leads')
plt.xlabel('City')
plt.ylabel('Proportion of Converted leads')
plt.savefig("city vs target")


# - from above all cities have similar conversion rate.

# ## airline loyality analysis

# In[55]:


b=df.airline_loyalty_tier.unique()


# In[56]:


print(df[df.Converted_y_N==1].airline_loyalty_tier.value_counts())


# In[57]:


print(df[df.Converted_y_N==0].airline_loyalty_tier.value_counts())


# In[58]:


sns.countplot(x=df.Converted_y_N,hue=df.airline_loyalty_tier)


# - from above plot no loyalty members has higher rate for both converting and non converting.

# In[59]:


for i in b:
    print(i)
    print(df[df.airline_loyalty_tier==i].Converted_y_N.value_counts(normalize=True))
    print("\n")
    


# In[60]:


table=pd.crosstab(df.airline_loyalty_tier,df.Converted_y_N)
table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)
plt.title('Stacked Bar Chart for airline loyality vs Converted leads')
plt.xlabel('airline membership')
plt.ylabel('Proportion of Converted leads')
plt.savefig("loyality vs target")


# - From the above analysis :
#     - Users with any one of the loyality plans have a 50% higher probality for conversion with 'base' plan with the highest conversion rate.
#     - Users without any loyality plans also have a positive conversion rate
#     - We  use this feature also for Customer value calculation.

# ## family member analysis

# In[61]:


print(df[df.Converted_y_N==1].no_of_family_members.value_counts())


# In[62]:


print(df[df.Converted_y_N==0].no_of_family_members.value_counts())


# In[63]:


sns.countplot(x=df.Converted_y_N,hue=df.no_of_family_members)


# In[64]:


c=df.no_of_family_members.unique()


# In[65]:


for i in c:
    print(i)
    print(df[df.no_of_family_members==i].Converted_y_N.value_counts(normalize=True))
    print("\n")
    


# In[66]:


table=pd.crosstab(df.no_of_family_members,df.Converted_y_N)
table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)
plt.title('Stacked Bar Chart for family members vs Converted leads')
plt.xlabel("Family members")
plt.ylabel('Proportion of Converted leads')
plt.savefig("family members vs target")


# - the family which has 4 members have the 100% probability of conversion rate.
# - use this value for customer value generation.

# ## 6M prior enquiry analysis

# In[67]:


print(df[df.Converted_y_N==1].is_6M_enquiry.value_counts(normalize=True))


# In[68]:


print(df[df.Converted_y_N==0].is_6M_enquiry.value_counts(normalize=True))


# In[69]:


print(df[df.is_6M_enquiry==1].Converted_y_N.value_counts(normalize=True))


# In[70]:


print(df[df.is_6M_enquiry==0].Converted_y_N.value_counts(normalize=True))


# In[71]:


table=pd.crosstab(df.is_6M_enquiry,df.Converted_y_N)
table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)
plt.title('Stacked Bar Chart for prior enquiry of 6months vs Converted leads')
plt.xlabel('6M enquiry')
plt.ylabel('Proportion of Converted leads')
plt.savefig("6m vs target")


#  - from the above analysis we can also find out that the enquiry affects the conversion to a certain degree
# - leads are converted even they are not enquiry in last 6 months
# - We can see that nearly 37% of the leads who visited the website in the last 6 months have converted. This is a very positive sign for conversion and we can use this in the process of lead value processing. 

# ## recent activity analysis

# In[72]:


print(df[df.Converted_y_N==1].is_3M_active.value_counts())


# In[73]:


print(df[df.Converted_y_N==0].is_3M_active.value_counts())


# In[74]:


print(df[df.is_3M_active==0].Converted_y_N.value_counts(normalize=True))


# In[75]:


print(df[df.is_3M_active==1].Converted_y_N.value_counts(normalize=True))


# In[76]:


table=pd.crosstab(df.is_3M_active,df.Converted_y_N)
table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)
plt.title('Stacked Bar Chart for last 3Month activity vs Converted leads')
plt.xlabel('City')
plt.ylabel('Proportion of Converted leads')
plt.savefig("3m vs target")


# - from above we infer the recent app not more important for conversion rates

# ## Analysis Insights
# 
#    - our target variable have imbalanced
#    
#   
#    - most of leads converted between the month of 3 to 5 ,1st month has the best conversion rate it has 100% probability.after this conversion rate reduce and maintain the average of 20% prob
#    
#    
#    - From the network channels, we are getting many leads but its conversion rate is less. But the direct channel leads are converting in high compared with network.
#    
#    
#    - From the male pupils, we are getting the leads in high number. But the conversion rate is higher in the leads from female have 50% more to conversion
#    
#    
#    - The leads are generated in less numbers from the city Cochin and Kolkata. And High no of leads are getting from the Surat and Bangalore.but conversion rate of cities are similar
#    
#    
#    - the people who doesn't have loyalty tier have the higher rate of  converting  it has positive conversion rate
#    
#    
#    - 4 no of family members have the 100% conversion rate
#    
#    
#    - the leads are converted even they didn't make last 6 months enquiry
#    
#    
#    - More or less similar amount of leads is converted either the peoples are active or not active in the websites within the past 3 months

# **treating categorical variables**

# In[95]:


df['lead_gen_channel'] = df.lead_gen_channel.replace(['Direct','Networks','Organic'],[0,1,2])
df['Gender'] = df.Gender.replace(['Female','Male'],[0,1])
df['City'] = df.City.replace(
    ['Ahmedabad','Bangalore','Cochin','Chandigarh','Chennai','Delhi','Hyderabad','Kolkata','Mumbai','New Delhi','Surat'],
    [0,1,2,3,4,5,6,7,8,9,10])
df['airline_loyalty_tier'] = df.airline_loyalty_tier.replace(
    ['None','Base','Silver','Gold','Platinum',],[0,1,2,3,4])


# **outliers treatment**

# In[96]:


df.Age=np.log(df.Age)


# In[99]:


sns.distplot(df.Age)


# ## base model

# In[100]:


df1=df.drop(['RowNumber','LeadId', 'Surname','age_band'],axis=1)


# In[101]:


df1.head()


# In[102]:


from sklearn.model_selection import train_test_split


# In[103]:


x=df1.drop(['Converted_y_N'], axis=1)
y=df1.Converted_y_N


# In[104]:


xtrain,xtest,ytrain,ytest=train_test_split(x,y, test_size=0.3,random_state=0)


# In[105]:


import xgboost as xgb


# In[106]:


from xgboost.sklearn import XGBClassifier
bst=XGBClassifier()
bst.fit(xtrain,ytrain)


# In[107]:


y1=bst.predict(xtest)


# In[108]:


from sklearn import metrics
from sklearn.metrics import roc_auc_score


# In[109]:


cm_xg=metrics.classification_report(ytest,y1)
acc_xg=metrics.accuracy_score(ytest,y1)
auc_xg=roc_auc_score(ytest,y1)
print(cm_xg)
print(acc_xg)
print(auc_xg)


# ## using smote

# In[110]:


from imblearn.over_sampling import SMOTE


# In[111]:


sm = SMOTE(random_state=2)
X_train, y_train = sm.fit_sample(xtrain, ytrain)


# In[112]:


y_train=pd.DataFrame(data=y_train,columns=["Converted_y_N"])


# **Find the important features by Recursive Feature Elimination**

# In[117]:


logreg = LogisticRegression()
rfe = RFE(logreg, 20)
rfe = rfe.fit(X_train, y_train.values.ravel())
print(rfe.support_)
print(rfe.ranking_)


# In[118]:


cols=['months_since_lead_gen', 'lead_gen_channel', 'Gender', 'Age', 'City',
       'airline_loyalty_tier', 'no_of_family_members', 'is_6M_enquiry',
       'is_3M_active'] 
X=X_train[cols]
y=y_train['Converted_y_N']


# In[121]:


logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary2())


# **traintest split**

# In[122]:


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)


# **Define various Models**

# **Logistic Regression**

# In[123]:


LR = LogisticRegression()


# **Bagged Logistic Regression**

# In[124]:


LR_Bagged = BaggingClassifier(base_estimator=LR,random_state=0)


# In[127]:


KF1 = KFold(n_splits=5,shuffle=True,random_state=0)
KF1_params ={'n_estimators':np.arange(1,10)}
LGS = RandomizedSearchCV(LR_Bagged,KF1_params,cv=KF1,scoring='roc_auc')
LGS.fit(X,y)


# In[136]:


LGS_bestparams = LGS.best_params_
LGS_n_estimators = LGS_bestparams['n_estimators']


# In[137]:


After_KFold_LR_Bagged = BaggingClassifier(base_estimator=LR,n_estimators=LGS_n_estimators,random_state=0)


# **ADA Boosted Logistic Regression**

# In[138]:


LR_Boost = AdaBoostClassifier(base_estimator=LR,n_estimators=50)


# **Naive-Bayes**

# In[139]:


NB = GaussianNB()
#ADA Boosted
NB_Boost = AdaBoostClassifier(base_estimator=NB,n_estimators=50)


# **Decision Tree**

# In[140]:


#DT - criterion='gini'
DT_model_Gini = DecisionTreeClassifier(random_state=0) #Full grown tree for shown the bias and variance
DT_model_Gini_Regularized = DecisionTreeClassifier(max_depth=4,random_state=0) #DT_Regularized

#DT - criterion='entropy'
DT_model_entropy = DecisionTreeClassifier(criterion='entropy',random_state=0) #Full grown tree for shown the bias and variance
DT_model_entropy_Regularized = DecisionTreeClassifier(max_depth=4,criterion='entropy',random_state=0) #DT_Regularized

#Bagged
DT_Bagged = BaggingClassifier(n_estimators=10,random_state=0) #Bagged

#Ada Boosted
DT_ADABoosted = AdaBoostClassifier(n_estimators=50)

#Gradient Boosted
GB_Boost = GradientBoostingClassifier(n_estimators=100)


# **K - Nearest Neighbour**

# In[141]:


KNN = KNeighborsClassifier() 
KNN_1 = KNeighborsClassifier(n_neighbors=5,weights='uniform') #Default


# In[142]:


params ={'n_neighbors':np.arange(1,50),'weights':['uniform','distance']}
#GS = GridSearchCV(KNN,params,cv=5,scoring='recall')
GS = RandomizedSearchCV(KNN,params,cv=5,scoring='roc_auc')
GS.fit(X,y)


# In[143]:


bestparams = GS.best_params_ 
KNN_n_neighbors = bestparams['n_neighbors']
KNN_weights = bestparams['weights']
KNN_2 = KNeighborsClassifier(n_neighbors=KNN_n_neighbors,weights=KNN_weights)


# In[144]:


#Bagged-KNN model
KNN_Bagged = BaggingClassifier(base_estimator=KNN_2) 


# In[145]:


KNN_Bagged_params ={'n_estimators':np.arange(1,50)}
GS1 = RandomizedSearchCV(KNN_Bagged,KNN_Bagged_params,cv=5,scoring='roc_auc')
GS1.fit(X,y)


# In[146]:


GS1_bestparams = GS1.best_params_
KNN_Bagged_n_estimators = GS1_bestparams['n_estimators']


# In[147]:


KNN_Bagged_with_n_estimators = BaggingClassifier(base_estimator=KNN_2,n_estimators=KNN_Bagged_n_estimators,random_state=0) 


# In[148]:


#getting the diff n_estimators, we using kfold for getting the same n_estimators
KF = KFold(n_splits=5,shuffle=True,random_state=0)
KF_params ={'n_estimators':np.arange(1,50)}
GS2 = RandomizedSearchCV(KNN_Bagged,KF_params,cv=KF,scoring='roc_auc')
GS2.fit(X,y)


# In[149]:


GS2_bestparams = GS2.best_params_
KF_n_estimators = GS2_bestparams['n_estimators']


# In[150]:


After_KF_KNN_Bagged = BaggingClassifier(base_estimator=KNN_2,n_estimators=KF_n_estimators,random_state=0) 


# **Random Forest**

# In[151]:


RF_gini = RandomForestClassifier(n_estimators=51,criterion='gini',random_state=0 )
RF_entropy = RandomForestClassifier(n_estimators=51,criterion='entropy',random_state=0 )

#ADA Boosted
RF_gini_ADABoosted = AdaBoostClassifier(base_estimator=RF_gini,n_estimators=50)
RF_entropy_ADABoosted = AdaBoostClassifier(base_estimator=RF_entropy,n_estimators=50)


# **Model Comparison**

# In[152]:


models = []
#Logistic Regression
models.append(('Logistic Regression', LR))
models.append(('Bagged Logistic Regression', LR_Bagged))
models.append(('After KFold Logistic Regression',After_KFold_LR_Bagged))
models.append(('ADA Boosted Logistic Regression', LR_Boost))
#Naive Bayes
models.append(('Naive-Bayes',NB))
models.append(('ADA Boosted Naive-Bayes',NB_Boost))
#Decision Tree
models.append(('Decision Tree - Gini',DT_model_Gini))
models.append(('Decision Tree - Gini Regularized',DT_model_Gini_Regularized))
models.append(('Decision Tree - Entropy',DT_model_entropy))
models.append(('Decision Tree - Entropy Regularized',DT_model_entropy_Regularized))
models.append(('Bagged Decision Tree',DT_Bagged))
models.append(('ADA Boosted Decision Tree',DT_ADABoosted))
models.append(('Gradient Boosted Decision Tree',GB_Boost))
#K-Nearest Neighbor
models.append(('KNN',KNN))
models.append(('KNN (Default)',KNN_1))
models.append(('KNN with best n-neighbor and weights',KNN_2))
models.append(('Bagged KNN',KNN_Bagged))
models.append(('After KFold Bagged KNN',After_KF_KNN_Bagged))
#Random Forest
models.append(('RF - Gini',RF_gini))
models.append(('RF - Entropy',RF_entropy))
models.append(('ADA Boosted RF - Gini',RF_gini_ADABoosted))
models.append(('ADA Boosted RF - Entropy',RF_entropy_ADABoosted))


# In[153]:


kf=KFold(n_splits=5,shuffle=True,random_state=0)
Result = pd.DataFrame(columns = ['Algorithm' , 'Accuracy', 'AUC Score' , 'FN Predictions','Avg Bias','Bias Error',
                                 'Avg Variance'])
for name, model in models:
    
    k=0
    recall=np.zeros((2,5))
    prec=np.zeros((2,5))
    fscore=np.zeros((2,5))
    
    algorithm = name
    X = X_train
    y = y_train
    for train,test in kf.split(X,y):
        Xtrain,Xtest=X.iloc[train,:],X.iloc[test,:]
        Ytrain,Ytest=y.iloc[train],y.iloc[test]
        model.fit(Xtrain,Ytrain)
        pred_y=model.predict(Xtest)
        confusionMatrix=metrics.confusion_matrix(Ytest,pred_y)
        for i in np.arange(0,2):
            recall[i,k]=confusionMatrix[i,i]/confusionMatrix[i,:].sum()
        for i in np.arange(0,2):
            prec[i,k]=confusionMatrix[i,i]/confusionMatrix[:,i].sum()
        k=k+1
    for row in np.arange(0,2):
        for col in np.arange(0,5):
            fscore[row,col]=2*(recall[row,col]*prec[row,col])/(recall[row,col]+prec[row,col])
    
 
    accuracy = metrics.accuracy_score(Ytest,pred_y)
    accuracy_value = round(accuracy*100,2)
    
 
    fpr,tpr, _ = roc_curve(Ytest,pred_y)
    roc_auc = auc(fpr, tpr)
    FNP = confusionMatrix[1][0]
    
    avg_f1=(np.mean(fscore[0,:])+np.mean(fscore[1,:]))/2
    avg_var=(np.var(fscore[0,:],ddof=1)+ np.var(fscore[1,:],ddof=1))/2
    
    Result = Result.append(pd.Series([algorithm, accuracy_value, roc_auc, FNP, avg_f1, 1-avg_f1,avg_var, ], 
                                     index=Result.columns ), ignore_index=True)


# In[154]:


Result


# In[155]:


Result.sort_values(['Accuracy'], ascending=False)


# **Final Prediction Model - Random Forest**

# In[158]:


X = X_train
y = y_train
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)

model = RandomForestClassifier()
params = {'criterion' : ['gini', 'entropy'], 'n_estimators' : np.arange(1,300),
            'min_samples_split' : [0.01, 0.001],
           'max_depth' : np.arange(1,50), 'max_features' : ['auto', 'sqrt', 'log2'],
          }
rsearch = RandomizedSearchCV(model, params, cv=5, n_iter=10, n_jobs=5, scoring='accuracy', verbose=True)
rsearch.fit(X,y)


# In[159]:


y_pred = rsearch.predict(X_test)
print("Accuracy: {} ".format(accuracy_score(y_test,y_pred) * 100))
#Classification Report
print(classification_report(y_test, y_pred))


# In[160]:


# save the model to disk
filename = 'finalized_model.sav'
pickle.dump(model, open(filename, 'wb'))


# In[ ]:




